{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Data Preprocessing\n",
    "Eileen McGivney  \n",
    "Student Science Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: 15 7th grade Student interviews, .txt files  \n",
    "Location:/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/S435Final/transcripts  \n",
    "Only student speech included, interviewer questions have been removed  \n",
    "Preprocessing: load in data, clean from punctuation, returns, stop words, etc. Make each transcript into segments. Load segments to vectors.   \n",
    "To start: pretty much replicate Sherin but then play around with different definitions (how to overlap text, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%cd '/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./unicode/amina.txt', './unicode/mike.txt', './unicode/maya.txt', './unicode/kyle.txt', './unicode/jenny.txt', './unicode/james.txt', './unicode/ben.txt', './unicode/simon.txt', './unicode/sasha.txt', './unicode/samuel.txt', './unicode/nicole.txt', './unicode/michelle.txt', './unicode/leila.txt', './unicode/emma.txt', './unicode/daniel.txt']\n"
     ]
    }
   ],
   "source": [
    "# using glob, find all the text files in the \"Papers\" folder\n",
    "import glob\n",
    "\n",
    "files = glob.glob('./unicode/*.txt')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought the whole thing was pretty fun and I like doing the tests that we ran. Some of them are kind of confusing though like the one that was outside that one I didn't really understand. But it was really fun, there were fun things like it was fun to explore even if we didn't go in or around some places it was fun to go around. \n",
      "It was fun looking for all the things in the water like certain different kinds of bacteria and things, I liked finding the things on land too. Like my partner and I we went through all of them, we made sure we could find all of them- even if we couldn't find it we made sure that we found it even if we had to do something else. Because it was fun to look for it, and I liked the information that they gave about it.\n",
      "I wrote a couple things, we didn't have much time but what I wrote was first I didn't think that fish needed that much to survive but then I realized that a lot of things affected it, like affected their lives. And another thing that's kind of similar to it I thought that only one thing would have kill the fish  but a lot of parts killed it.\n",
      "At first I thought it was only it dissolved oxygen. But then I realized that there were other parts that affected the dissolved oxygen that made the dissolved oxygen kill the fish so it wasn't just the dissolved oxygen it was other things too.\n",
      "I think of a scientist as someone who tries to find things out.\n",
      "They find evidence to t support what they're trying to learn and what they have learned to tell other people- so they can have new ideas about the world.\n",
      "I kind of thought that before but just confirmed what I felt about it because I knew that's what we were doing. And you learned new things along the way- I guess I didn't know that, that's what I learned.\n",
      "I don't know, I don't know if I do. I think my uncle is, I think my uncle is an earth scientist like what this is but I don't really know.\n",
      "When I was on vacation they were asking me what I was doing and I was explaining what I was doing and they were saying that's what his job was, so I can't remember that much but I know his job has to do with science, earth science, rock science. That kind of stuff.\n",
      "Yeah, probably less water-related though but like rocks, and land. But the kind of stuff we were doing is probably the kind of stuff that he does. \n",
      "Probably, like if I do- whatever I do I definitely want to include some part of science or math or something. I don’t know if I want to be just a scientist but I want to have it be a part of it. \n",
      "Well I want to be an author. And so maybe I could write books that have to do with science. Not factual books, but fictional books that include parts of science.\n",
      "I can’t think of anything right now but maybe if there was a story where someone lost someone in the pond, or an animal in the pond and they had to learn certain things to find where the animal could have been, and then they- learned more things about the pond. \n",
      "I think that the way I learned how to use evidence, to back up claims and where I can find certain things, that was really helpful. So I’ll remember that.  \n",
      "Maybe for other science projects or if I end up using science in my life, this helped me learn how to take evidence and put my reasoning with it. \n",
      "I think they have just learned about many different parts of science, different chemicals or certain parts to it, or they have practiced things like EcoXPT many times. \n",
      "I think so, at least I think that was a good start\n",
      "I like the test- EcoXPT was really fun but I like when we do actually test things in real life. It’s fun to realize that you found something out. Like EcoXPT is kind of like that, but just in real life too.\n",
      "I know the other classes were doing things with water, like if you’re trying to find things in water and I think that would be really fun to see what’s inside certain kinds of water.\n",
      "\n",
      "It had a lot more- I don’t know what the word it- it had a lot more than just an assignment. This makes sense technology-wise but also science-wise it was an entire different world. It wasn’t just a project it felt more like you were learning a whole different part of life than you know. \n",
      "Like most of the things that we knew that affected the fish’s lives, I would have never known if we didn’t do EcoXPT. I would have never known what those materials or ingredients were or whatever it is. I wouldnt’ have known what phosphates was, I wouldn’t have known that dissolved oxygen had anything to do with it. And so I just felt like I didn’t know about any of that stuff and after I did EcoXPT I knew a lot more about just that whole area of water and fish…\n",
      "Right now I don’t think it is, but if I went to the pond and I saw fish, I just could see other things inside the pond or imagine what’s inside it now and understand more about it because of EcoXPT. \n",
      "I think we got to really see things, because even if we had done it in real life we wouldn’t have gotten to understand what it looks like and how it worked by seeing it, because even though it wasn’t real it really showed how it would have worked if it was real. \n",
      "I think when I did the experiments and looked at the graph it helped me learn more things about how things went up and down or what other things affected it.\n",
      "Yeah even if it doesn’t have to do with science it still is a different way of looking at things, testing and the graphs. \n",
      "Anything, I mean graphs could be anything and I can see how different things affect things. Like with the ice cream thing we did, the people who drown and the ice cream. Like it doesn’t have to do with science but it’s still one thing affecting the other. Or what was the other- graphs or… oh testing. Well I guess it’s kind of hard to do testing that doesn’t have to do with science. I can’t think of another way you can do testing, but I know there are other ways.\n",
      "I don’t know, there were some things like I wanted to look inside the house and see other, more things. For it, I don’t think anything should change. \n",
      "I know some people who just didn’t try, and that’s because they didn’t want to try, but if they had tried they would have liked it. And everyone else pretty much tried and I think they liked it. \n",
      "It definitely made me like science more. Science has never been my favorite subject but it was more fun to just- we don’t usually talk about water or animals as much as I would like to so when we moved onto this part I liked it more. \n",
      "We usually do things- we did rock science, we did a unit on systems, we did energy like light bulbs and that kind of stuff, but I like things that are living more than things like electricity. \n",
      "Yeah I think it was a fun experience. For some people who already knew this stuff I think they still liked it, because they saw a different way of doing it. But I think it would be fun for most people, people who try. \n",
      "I think one thing- this might just be the people who didn’t try- found it boring, I don’t know how but some people did. I don’t really know how they would because there was so much to do, but some people said they found it boring. I didn’t think it was boring. \n",
      "I don’t know, I haven’t talked to many people about- I got to talk to more people who weren’t doing it because I wanted to know what they were doing. But I don’t know how many people found it boring. Maybe it’s just not their kind of thing. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get all the data from the text files into the \"documents\" list\n",
    "\n",
    "documents = []\n",
    "\n",
    "for filename in files: \n",
    "    with open (filename, \"r\", encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace carriage returns (i.e., \"\\n\") with a white space\n",
    "\n",
    "documents = [doc.replace('\\n', ' ') for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I thought it was pretty fun and it s like a different way to learn and also it s more hands on without actually going into the real world   Other things are like paper and it s written  and you don t really get to see everything how to choose like where you go and what you do  You get to go and make your own comparison tank and make your own hypothesis So I wrote down that science is easy and it s more like one branch  and that it only goes one direction  Now I m more realizing that it s able to go off in different hypothises  hypotheses  different areas you can go to   A scientist conducts research to make hypotheses about what s going on in the real world  Well now more the real world part  it s kind of nt something I realized more   since it s actually more Hands On than behind a screen doing a test  Well what I ve heard from other kids that are in classes that don t do EcoXPT  You got this worksheet that you have to figure out of different characters who contaminated the water or \n"
     ]
    }
   ],
   "source": [
    "# replace the punctation below by a white space\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’', '…', '—']\n",
    "\n",
    "\n",
    "# remove ponctuation\n",
    "for i,doc in enumerate(documents): \n",
    "    for punc in punctuation: \n",
    "        doc = doc.replace(punc, ' ')\n",
    "    documents[i] = doc\n",
    "    \n",
    "print(documents[1][:1000])\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I thought it was pretty fun and it s like a different way to learn and also it s more hands on without actually going into the real world   Other things are like paper and it s written  and you don t really get to see everything how to choose like where you go and what you do  You get to go and make your own comparison tank and make your own hypothesis So I wrote down that science is easy and it s more like one branch  and that it only goes one direction  Now I m more realizing that it s able to go off in different hypothises  hypotheses  different areas you can go to   A scientist conducts research to make hypotheses about what s going on in the real world  Well now more the real world part  it s kind of nt something I realized more   since it s actually more Hands On than behind a screen doing a test  Well what I ve heard from other kids that are in classes that don t do EcoXPT  You got this worksheet that you have to figure out of different characters who contaminated the water or \n"
     ]
    }
   ],
   "source": [
    "# remove numbers by either a white space\n",
    "for i,doc in enumerate(documents): \n",
    "    for num in range(10):\n",
    "        doc = doc.replace(str(num), '')\n",
    "    documents[i] = doc\n",
    "\n",
    "print(documents[1][:1000])\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought the whole thing was pretty fun and I like doing the tests that we ran  Some of them are ki\n",
      "[('i', 458), ('the', 393), ('to', 375), ('and', 362), ('it', 348), ('of', 234), ('a', 234), ('that', 208), ('we', 180), ('t', 166), ('was', 149), ('you', 149), ('like', 142), ('really', 138), ('they', 136), ('in', 134), ('but', 121), ('things', 112), ('think', 105), ('what', 103), ('have', 101), ('do', 96), ('s', 91), ('about', 74), ('if', 73), ('different', 73), ('not', 69), ('there', 68), ('know', 67), ('lot', 66), ('be', 66), ('how', 65), ('because', 64), ('don', 63), ('all', 62), ('science', 61), ('just', 60), ('yeah', 59), ('more', 59), ('or', 57), ('are', 55), ('on', 55), ('would', 54), ('can', 52), ('so', 51), ('is', 51), ('kind', 50), ('my', 49), ('at', 49), ('for', 48)]\n"
     ]
    }
   ],
   "source": [
    "# what stop words should we consider?\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from collections import Counter\n",
    "\n",
    "#combine docs into one\n",
    "alldocs = ' '.join(documents)\n",
    "print(alldocs[0:100])\n",
    "\n",
    "# # Tokenize the article: tokens\n",
    "tokens = nltk.word_tokenize(alldocs)\n",
    "\n",
    "# # Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# # Create a Counter with the lowercase tokens\n",
    "word_count = Counter(lower_tokens)\n",
    "\n",
    "# # Print the 50 most common tokens\n",
    "print(word_count.most_common(50))\n",
    "\n",
    "### Want to keep some pronouns: me, my, myself and they, them, themselves -- are they talking more about themselves or others?\n",
    "##Also keeping words like science, think and different. Will remove \"yeah\", \"yes\" and \"just\"\n",
    "##Generally list from before looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'thought', 'it', 'was', 'pretty', 'fun', 'and', 'it', 's', 'like', 'a', 'different', 'way', 'to', 'learn', 'and', 'also', 'it', 's', 'more', 'hands', 'on', 'without', 'actually', 'going', 'into', 'the', 'real', 'world', 'other', 'things', 'are', 'like', 'paper', 'and', 'it', 's', 'written', 'and', 'you', 'don', 't', 'really', 'get', 'to', 'see', 'everything', 'how', 'to', 'choose', 'like', 'where', 'you', 'go', 'and', 'what', 'you', 'do', 'you', 'get', 'to', 'go', 'and', 'make', 'your', 'own', 'comparison', 'tank', 'and', 'make', 'your', 'own', 'hypothesis', 'so', 'i', 'wrote', 'down', 'that', 'science', 'is', 'easy', 'and', 'it', 's', 'more', 'like', 'one', 'branch', 'and', 'that', 'it', 'only', 'goes', 'one', 'direction', 'now', 'i', 'm', 'more', 'realizing']\n",
      "['it', 'was', 'fun', 'collecting', 'data', 'i', 'm', 'not', 'really', 'sure', 'i', 'didn', 't', 'really', 'have', 'any', 'i', 'don', 't', 'know', 'basically', 'just', 'tried', 'to', 'figure', 'out', 'things', 'about', 'the', 'world', 'not', 'really', 'good', 'way', 'to', 'test', 'hypotheses', 'yeah', 'i', 'forgot', 'not', 'really', 'not', 'really', 'maybe', 'i', 'don', 't', 'know', 'it', 'lets', 'you', 'know', 'more', 'about', 'the', 'world', 'because', 'i', 'prefer', 'math', 'maybe', 'i', 'don', 't', 'know', 'it', 'could', 'it', 'could', 'help', 'you', 'figure', 'out', 'why', 'historical', 'events', 'happened', 'wanting', 'to', 'know', 'more', 'having', 'questions', 'that', 'they', 'want', 'to', 'answer', 'research', 'instead', 'of', 'just', 'saying', 'i', 'wonder', 'why', 'this', 'is', 'true']\n"
     ]
    }
   ],
   "source": [
    "#make lowercase\n",
    "tokendocs = [nltk.word_tokenize(doc) for doc in documents]\n",
    "lowerdocs = [[word.lower() for word in doc] for doc in tokendocs]\n",
    "\n",
    "print(lowerdocs[1][:100])\n",
    "print(lowerdocs[5][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lowerdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497\n",
      "964\n",
      "665\n",
      "325\n",
      "1472\n",
      "188\n",
      "341\n",
      "1082\n",
      "997\n",
      "631\n",
      "1030\n",
      "379\n",
      "663\n",
      "384\n",
      "469\n"
     ]
    }
   ],
   "source": [
    "for doc in lowerdocs:\n",
    "    print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before length of doc 5:188\n",
      "before length of doc 5:75\n"
     ]
    }
   ],
   "source": [
    "# Remove the stop words below from our documents\n",
    "\n",
    "stop_words = ['i', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'their', 'theirs',  \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now', 'yeah', 'just', 'yes', \n",
    "             'll', 've', 'm', 'lot', 'kind', 'well', 're', 'd', 'one', 'like', \n",
    "             'really', 'stuff']\n",
    "\n",
    "\n",
    "print('before length of doc 6:'+str(len(lowerdocs[5])))\n",
    "for i,doc in enumerate(lowerdocs):\n",
    "    for stopword in stop_words:\n",
    "        doc = list(filter(lambda word: word != stopword, doc))\n",
    "    lowerdocs[i]=doc\n",
    "print('before length of doc 6:'+str(len(lowerdocs[5])))\n",
    "\n",
    "docs=lowerdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'whole', 'thing', 'pretty', 'fun', 'tests', 'ran', 'them', 'confusing', 'though', 'outside', 'didn', 'understand', 'fun', 'fun', 'things', 'fun', 'explore', 'even', 'didn', 'go', 'around', 'places', 'fun', 'go', 'around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote', 'first', 'didn', 'think', 'fish', 'needed', 'much', 'survive', 'realized', 'things', 'affected', 'affected', 'lives', 'another', 'thing', 'similar', 'thought', 'thing', 'would', 'kill', 'fish', 'parts', 'killed', 'first', 'thought', 'dissolved', 'oxygen', 'realized', 'parts', 'affected', 'dissolved']\n",
      "[('they', 136), ('things', 112), ('think', 105), ('different', 73), ('know', 67), ('science', 61), ('would', 54), ('my', 49), ('could', 47), ('see', 44), ('didn', 43), ('fish', 43), ('fun', 40), ('go', 39), ('people', 38), ('something', 37), ('experiments', 33), ('look', 32), ('try', 32), ('thought', 31), ('find', 31), ('good', 31), ('me', 28), ('got', 27), ('around', 26), ('want', 26), ('world', 25), ('take', 25), ('thing', 24), ('learn', 24), ('ecoxpt', 24), ('water', 23), ('time', 23), ('oxygen', 23), ('evidence', 23), ('get', 23), ('dissolved', 22), ('scientist', 22), ('might', 22), ('scientists', 21), ('them', 20), ('maybe', 19), ('use', 19), ('problem', 19), ('lab', 19), ('liked', 18), ('guess', 18), ('part', 18), ('actually', 18), ('real', 18)]\n",
      "4485\n"
     ]
    }
   ],
   "source": [
    "# # Create a Counter for all the words across the documents\n",
    "\n",
    "alldocs = [y for x in docs for y in x]\n",
    "print(alldocs[:100])\n",
    "word_count = Counter(alldocs)\n",
    "\n",
    "# # Print the 50 most common tokens\n",
    "print(word_count.most_common(50))\n",
    "print(len(alldocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the cleaned transcripts for bag-of-words analysis on the individual documents\n",
    "import pickle\n",
    "for i,doc in enumerate(docs):\n",
    "    with open('./cleandocs/doc'+str(i)+'.txt', 'wb') as fp:\n",
    "        pickle.dump(doc, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n",
      "['ability', 'able', 'access', 'accurate', 'accused', 'across', 'actually', 'add', 'added', 'adult', 'advantage', 'affect', 'affected', 'affecting', 'affects', 'ago', 'ahead', 'aimlessly', 'air', 'algae', 'almost', 'along', 'already', 'also', 'although', 'always', 'amount', 'analyze', 'animal', 'animals', 'another', 'answer', 'answered', 'answering', 'answers', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'apiologist', 'archaeologists', 'area', 'areas', 'aren', 'arguing', 'around', 'arrangement', 'ask', 'asking']\n",
      "Stored 'vocabulary' (list)\n"
     ]
    }
   ],
   "source": [
    "# use our function that takes in a list of documents\n",
    "# and returns a set of unique words.\n",
    "\n",
    "def get_vocabulary(lists):\n",
    "    voc = []\n",
    "    for document in lists:\n",
    "        for word in document:\n",
    "            if word not in voc: \n",
    "                voc.append(word)\n",
    "    voc = list(set(voc))\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "vocabulary = get_vocabulary(docs)\n",
    "print(len(vocabulary))\n",
    "print(vocabulary[:50])\n",
    "\n",
    "%store vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 100 word segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'whole', 'thing', 'pretty', 'fun', 'tests', 'ran', 'them', 'confusing', 'though', 'outside', 'didn', 'understand', 'fun', 'fun', 'things', 'fun', 'explore', 'even', 'didn', 'go', 'around', 'places', 'fun', 'go', 'around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote', 'first', 'didn', 'think', 'fish', 'needed', 'much', 'survive', 'realized', 'things', 'affected', 'affected', 'lives', 'another', 'thing', 'similar', 'thought', 'thing', 'would', 'kill', 'fish', 'parts', 'killed', 'first', 'thought', 'dissolved', 'oxygen', 'realized', 'parts', 'affected', 'dissolved']\n",
      "['around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote', 'first', 'didn', 'think', 'fish', 'needed', 'much', 'survive', 'realized', 'things', 'affected', 'affected', 'lives', 'another', 'thing', 'similar', 'thought', 'thing', 'would', 'kill', 'fish', 'parts', 'killed', 'first', 'thought', 'dissolved', 'oxygen', 'realized', 'parts', 'affected', 'dissolved', 'oxygen', 'made', 'dissolved', 'oxygen', 'kill', 'fish', 'wasn', 'dissolved', 'oxygen', 'things', 'think', 'scientist', 'someone', 'tries', 'find', 'things', 'they', 'find', 'evidence', 'support', 'they', 'trying', 'learn', 'they', 'learned']\n",
      "['find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote', 'first', 'didn', 'think', 'fish', 'needed', 'much', 'survive', 'realized', 'things', 'affected', 'affected', 'lives', 'another', 'thing', 'similar', 'thought', 'thing', 'would', 'kill', 'fish', 'parts', 'killed', 'first', 'thought', 'dissolved', 'oxygen', 'realized', 'parts', 'affected', 'dissolved', 'oxygen', 'made', 'dissolved', 'oxygen', 'kill', 'fish', 'wasn', 'dissolved', 'oxygen', 'things', 'think', 'scientist', 'someone', 'tries', 'find', 'things', 'they', 'find', 'evidence', 'support', 'they', 'trying', 'learn', 'they', 'learned', 'tell', 'people', 'they', 'new', 'ideas', 'world', 'thought', 'confirmed', 'felt', 'knew', 'learned', 'new', 'things', 'along', 'way', 'guess', 'didn', 'know', 'learned', 'know', 'know', 'think', 'my', 'uncle', 'think']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "### like what we did in class but preserving each document as a separate one \n",
    "### Make 100-word segments w 25-word overlap like Sherin paper\n",
    "def segment_hundred(docs, window_size=100, overlap=25):\n",
    "    new_documents = []\n",
    "    for doc in docs:\n",
    "        segs = []\n",
    "        # create segments of 100 words\n",
    "        high = window_size\n",
    "        while high < len(doc):\n",
    "            low = high - window_size\n",
    "            segs.append(doc[low:high])\n",
    "            high += overlap\n",
    "        new_documents.append(segs)\n",
    "    return new_documents\n",
    "\n",
    "segs_hundred = segment_hundred(docs)\n",
    "print(segs_hundred[0][0])\n",
    "print(segs_hundred[0][1])\n",
    "print(segs_hundred[0][2])\n",
    "print(len(segs_hundred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the segments for visualization:\n",
    "import pickle\n",
    "with open('./cleandocs/segs_hundred.txt', 'wb') as fp:\n",
    "    pickle.dump(segs_hundred, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'whole', 'thing', 'pretty', 'fun', 'tests', 'ran', 'them', 'confusing', 'though', 'outside', 'didn', 'understand', 'fun', 'fun', 'things', 'fun', 'explore', 'even', 'didn', 'go', 'around', 'places', 'fun', 'go', 'around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn']\n",
      "['go', 'around', 'places', 'fun', 'go', 'around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote']\n",
      "['partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even', 'something', 'else', 'fun', 'look', 'liked', 'information', 'they', 'gave', 'wrote', 'couple', 'things', 'didn', 'much', 'time', 'wrote', 'first', 'didn', 'think', 'fish', 'needed', 'much', 'survive', 'realized', 'things', 'affected', 'affected', 'lives', 'another', 'thing', 'similar', 'thought', 'thing', 'would', 'kill', 'fish']\n"
     ]
    }
   ],
   "source": [
    "### Make 50 word segments with 30 word overlap\n",
    "\n",
    "def segment(docs, window_size, overlap):\n",
    "    new_documents = []\n",
    "    for doc in docs:\n",
    "        segs = []\n",
    "        \n",
    "        high = window_size\n",
    "        while high < len(doc):\n",
    "            low = high - window_size\n",
    "            segs.append(doc[low:high])\n",
    "            high += overlap\n",
    "        new_documents.append(segs)\n",
    "    return new_documents\n",
    "segs_fifty = segment(docs, 50, 20)\n",
    "print(segs_fifty[0][0])\n",
    "print(segs_fifty[0][1])\n",
    "print(segs_fifty[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the segments for visualization:\n",
    "import pickle\n",
    "with open('./cleandocs/segs_fifty.txt', 'wb') as fp:\n",
    "    pickle.dump(segs_fifty, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thought', 'whole', 'thing', 'pretty', 'fun', 'tests', 'ran', 'them', 'confusing', 'though', 'outside', 'didn', 'understand', 'fun', 'fun', 'things', 'fun', 'explore', 'even', 'didn', 'go', 'around', 'places', 'fun', 'go']\n",
      "['things', 'fun', 'explore', 'even', 'didn', 'go', 'around', 'places', 'fun', 'go', 'around', 'fun', 'looking', 'things', 'water', 'certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my']\n",
      "['certain', 'different', 'kinds', 'bacteria', 'things', 'liked', 'finding', 'things', 'land', 'my', 'partner', 'went', 'them', 'made', 'sure', 'could', 'find', 'them', 'even', 'couldn', 'find', 'made', 'sure', 'found', 'even']\n"
     ]
    }
   ],
   "source": [
    "### Make 25-word segments with 15 word overlap\n",
    "segs_twentyfive = segment(docs, 25, 15)\n",
    "print(segs_twentyfive[0][0])\n",
    "print(segs_twentyfive[0][1])\n",
    "print(segs_twentyfive[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "194\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "#Flatten the different list-of-lists into one list so they are all in one dataframe together\n",
    "#100-word segments list\n",
    "segs_hundred_flat=[]\n",
    "for i in segs_hundred:\n",
    "    for j in i:\n",
    "        segs_hundred_flat.append(j)\n",
    "print (len(segs_hundred_flat))\n",
    "\n",
    "#50-word segments list\n",
    "segs_fifty_flat=[]\n",
    "for i in segs_fifty:\n",
    "    for j in i:\n",
    "        segs_fifty_flat.append(j)\n",
    "print (len(segs_fifty_flat))\n",
    "\n",
    "\n",
    "#25-word segments list\n",
    "segs_twentyfive_flat=[]\n",
    "for i in segs_twentyfive:\n",
    "    for j in i:\n",
    "        segs_twentyfive_flat.append(j)\n",
    "print (len(segs_twentyfive_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del df_h\n",
    "# del df_f\n",
    "# del df_t\n",
    "# del df_h_log\n",
    "# del df_f_log\n",
    "# del df_t_log\n",
    "# del df_h_norm\n",
    "# del df_f_norm\n",
    "# del df_t_norm\n",
    "# del df_h_dev\n",
    "# del df_f_dev\n",
    "# del df_t_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#create a function that takes a list of documents\n",
    "# and a vocabulary as arguments, and returns a dataframe with the counts\n",
    "# of words: \n",
    "import pandas as pd\n",
    "def docs_by_words_df(segments, vocabulary):\n",
    "    df = pd.DataFrame(0, index=np.arange(len(segments)), columns=vocabulary)\n",
    "    \n",
    "    # fill out the matrix with counts\n",
    "    for i,seg in enumerate(segments):\n",
    "        for word in seg:\n",
    "            if word in df.columns: \n",
    "                df.loc[i,word] += 1\n",
    "            \n",
    "    return df\n",
    "## Create a dataframe for the 100-word segments [df_h]\n",
    "df_h = docs_by_words_df(segs_hundred_flat, vocabulary)\n",
    "print(df_h.loc[0,'around'])\n",
    "#print(df_h.head())\n",
    "\n",
    "## create a dataframe for the 50-word segments [df_f]\n",
    "df_f = docs_by_words_df(segs_fifty_flat, vocabulary)\n",
    "print(df_f.loc[0,'around'])\n",
    "\n",
    "\n",
    "## create a dataframe for the 25-word segments[df_t]\n",
    "df_t = docs_by_words_df(segs_twentyfive_flat, vocabulary)\n",
    "print(df_t.loc[0,'around'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weighting words - log transformation\n",
    "\n",
    "# Function for log transformation:\n",
    "def one_plus_log(cell):\n",
    "    if cell != 0: \n",
    "        return 1 + math.log(cell)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before one + log:  2\n",
      "after one + log:  1.6931471805599454\n",
      "Value in the dataframe:  1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "###Apply function to the three dataframes:\n",
    "\n",
    "df_h_log = df_h.applymap(one_plus_log)\n",
    "df_f_log = df_f.applymap(one_plus_log)\n",
    "df_t_log = df_t.applymap(one_plus_log)\n",
    "\n",
    "#check that the numbers in the resulting matrix look accurate;\n",
    "# print the value before and after applying the function above\n",
    "print(\"before one + log: \", df_h.loc[0,'around'])\n",
    "print(\"after one + log: \", 1 + math.log(df_h.loc[0,'around']))\n",
    "print(\"Value in the dataframe: \", df_h_log.loc[0,'around'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Normalize the dataframes\n",
    "\n",
    "#Function for normalizing\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "def normalize_df(df, method='Normalizer'):\n",
    "    \n",
    "    # choose the normalization strategy\n",
    "    scaler = None\n",
    "    if method == 'Normalizer': scaler = Normalizer()\n",
    "    if method == 'MinMaxScaler': scaler = MinMaxScaler()\n",
    "    if method == 'StandardScaler': scaler = StandardScaler()\n",
    "        \n",
    "    # apply the normalization\n",
    "    if scaler != None:\n",
    "        df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "\n",
    "    # return the resulting dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before normalizer:  1.6931471805599454\n",
      "Value in the dataframe after normalizer:  0.1523841709002107\n",
      "before normalizer:  1.6931471805599454\n",
      "Value in the dataframe after normalizer:  0.22163532825986462\n",
      "before normalizer:  1.0\n",
      "Value in the dataframe after normalizer:  0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "#Apply normalizer to the dataframes\n",
    "### 100-word segments:\n",
    "print(\"before normalizer: \", df_h_log.loc[0,'around'])\n",
    "df_h_norm = normalize_df(df_h_log, method='Normalizer')\n",
    "print(\"Value in the dataframe after normalizer: \", df_h_norm.loc[0,'around'])\n",
    "\n",
    "### 50-word segments:\n",
    "print(\"before normalizer: \", df_f_log.loc[0,'around'])\n",
    "df_f_norm = normalize_df(df_f_log, method='Normalizer')\n",
    "print(\"Value in the dataframe after normalizer: \", df_f_norm.loc[0,'around'])\n",
    "\n",
    "### 25-word segments:\n",
    "print(\"before normalizer: \", df_t_log.loc[0,'around'])\n",
    "df_t_norm = normalize_df(df_t, method='Normalizer')\n",
    "print(\"Value in the dataframe after normalizer: \", df_t_norm.loc[0,'around'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Normalized Dataframes for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalized vectors:\n",
    "df_h_norm.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_hundred.csv')\n",
    "df_f_norm.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_fifty.csv')\n",
    "df_t_norm.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_twentyfive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function to create deviation vectors\n",
    "\n",
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def length_norm(u):\n",
    "    return u / vector_length(u)\n",
    "\n",
    "def transform_deviation_vectors(df):\n",
    "    \n",
    "    # get the numpy matrix from the df\n",
    "    matrix = df.values\n",
    "    \n",
    "    # compute the sum of the vectors\n",
    "    v_sum = np.sum(matrix, axis=0)\n",
    "    \n",
    "    # normalize this vector (find its average)\n",
    "    v_avg = length_norm(v_sum)\n",
    "    \n",
    "    # we iterate through each vector\n",
    "    for row in range(df.shape[0]):\n",
    "        \n",
    "        # this is one vector (row\n",
    "        v_i = matrix[row,:]\n",
    "        \n",
    "        # we subtract its component along v_average\n",
    "        scalar = np.dot(v_i,v_avg)\n",
    "        sub = v_avg * scalar\n",
    "        \n",
    "        # we replace the row by the deviation vector\n",
    "        matrix[row,:] = length_norm(v_i - sub)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before vectorizer:  0.1523841709002107\n",
      "Value in the dataframe after vectorizer:  0.1376282239692348\n",
      "before vectorizer:  0.22163532825986462\n",
      "Value in the dataframe after vectorizer:  0.20915728224431457\n",
      "before vectorizer:  0.14285714285714285\n",
      "Value in the dataframe after vectorizer:  0.12861557191678286\n"
     ]
    }
   ],
   "source": [
    "### Apply the function and check values before and after for each dataframe:\n",
    "\n",
    "print(\"before vectorizer: \", df_h_norm.loc[0,'around'])\n",
    "df_h_dev = transform_deviation_vectors(df_h_norm)\n",
    "print(\"Value in the dataframe after vectorizer: \", df_h_dev.loc[0,'around'])\n",
    "\n",
    "print(\"before vectorizer: \", df_f_norm.loc[0,'around'])\n",
    "df_f_dev = transform_deviation_vectors(df_f_norm)\n",
    "print(\"Value in the dataframe after vectorizer: \", df_f_dev.loc[0,'around'])\n",
    "\n",
    "print(\"before vectorizer: \", df_t_norm.loc[0,'around'])\n",
    "df_t_dev = transform_deviation_vectors(df_t_norm)\n",
    "print(\"Value in the dataframe after vectorizer: \", df_t_dev.loc[0,'around'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_h_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save deviation vectors deataframes\n",
    "for using in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deviance vectors\n",
    "df_h_dev.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_hundred_deviance.csv')\n",
    "df_f_dev.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_fifty_deviance.csv')\n",
    "df_t_dev.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/segments_twentyfive_deviance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis see:  \n",
    "/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/Analysis-McGivney.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize full transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Use function from above to make a dataframe \n",
    "\n",
    "df_all = docs_by_words_df(docs, vocabulary)\n",
    "print(df_all.loc[0,'around'])\n",
    "#print(df_all.head())\n",
    "#print(df_all.loc[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before one + log:  2\n",
      "after one + log:  1.6931471805599454\n",
      "Value in the dataframe:  1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "###Apply the log function\n",
    "\n",
    "df_all_log = df_all.applymap(one_plus_log)\n",
    "\n",
    "\n",
    "#check that the numbers in the resulting matrix look accurate;\n",
    "# print the value before and after applying the function above\n",
    "print(\"before one + log: \", df_all.loc[0,'around'])\n",
    "print(\"after one + log: \", 1 + math.log(df_all.loc[0,'around']))\n",
    "print(\"Value in the dataframe: \", df_all_log.loc[0,'around'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before normalizer:  1.6931471805599454\n",
      "Value in the dataframe after normalizer:  0.06439224064355403\n"
     ]
    }
   ],
   "source": [
    "#Apply normalizer to the dataframe:\n",
    "\n",
    "print(\"before normalizer: \", df_all_log.loc[0,'around'])\n",
    "df_all_norm = normalize_df(df_all_log, method='Normalizer')\n",
    "print(\"Value in the dataframe after normalizer: \", df_all_norm.loc[0,'around'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Normalized Dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalized vectors:\n",
    "df_all_norm.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/transcripts_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before vectorizer:  0.06439224064355403\n",
      "Value in the dataframe after vectorizer:  -0.010979237528006825\n"
     ]
    }
   ],
   "source": [
    "### Apply the function for deviance vectors and check values before and after:\n",
    "\n",
    "print(\"before vectorizer: \", df_all_norm.loc[0,'around'])\n",
    "df_all_dev = transform_deviation_vectors(df_all_norm)\n",
    "print(\"Value in the dataframe after vectorizer: \", df_all_dev.loc[0,'around'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save deviation vectors deataframes\n",
    "for using in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deviance vectors\n",
    "df_all_dev.to_csv('/Volumes/GoogleDrive/My Drive/Spring 2019/S435/GitHub/final-project-EileenMcGivney/dataframes/transcripts_all_deviance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making vectors for the full transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "511px",
    "left": "836px",
    "right": "20px",
    "top": "69px",
    "width": "548px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
